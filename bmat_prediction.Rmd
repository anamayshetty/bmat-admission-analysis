---
title: "Predicting BMAT Results"
author: "Anamay Shetty"
date: "12/12/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rethinking)
theme_set(theme_minimal())
knitr::opts_chunk$set(echo = FALSE)
```

```{bash Converting to R-readable format, include = FALSE}
sed -e 's| Home |,Home |g'  -e 's|Y| Y|g' -e 's| *Y| Y|g' data/raw_bmat_results_tl.txt | tr "," "\n" > data/bmat_results_tl.txt
```

```{r Read in data file, include = FALSE}
df <- read_delim("data/bmat_results_tl.txt", delim = " ", col_names = c("Domicile", "Course", "Year", "Candidate", letters[1:8]), skip = 1) %>%
  unite("x", a:h, sep = " ") %>%
  mutate(x = str_remove_all(x, " NA")) %>%
  mutate(b = str_extract(x, "[0-9](.[0-9]|) [0-9](.[0-9]|)")) %>%
  separate(b, into = c("BMAT_Section_1", "BMAT_Section_2"), sep = " ", convert = TRUE) %>%
  mutate(Offer = ifelse(str_detect(x, "Y "), TRUE, FALSE)) %>%
  mutate(x = (str_remove(x, " ?(Y|) [0-9](.[0-9]|) [0-9](.[0-9]|)"))) %>%
  rename("College" = x) %>%
  mutate(BMAT_all = BMAT_Section_1 + BMAT_Section_2) %>%
  #group_by(College, Year) %>%
  #mutate(offer_fraction = sum(Offer)/n()) %>%
  #ungroup %>%
  select(-Domicile, -Course) %>%
  filter(College != "Hughes Hall")
```

# Introduction

This document is intended to show a simple Bayesian workflow, based off the methods from McElreath's Statistical Rethinking. 

# Question

The Bio-Medial Aptitude Test (BMAT) is an admissions test sat by students applying to 8 universities in the UK, including the University of Cambridgde. It is most commonly sat in November, and consists of three sections:

1. Section 1, which is a verbal/spatial reasoning section

2. Section 2, which is a section on science and maths based on the GCSE curriculum.

3. Section 3, which is an essay section.

The entire exam takes two hours, and candidates recieve a mark of 1-9 each for Section 1 and 2, and a mark from 1-5 for Section 3, along with a mark A-E for their spelling and grammar in the essay.

Candidates often want to know what their chances of getting into Cambridge are based on their BMAT score, and this generates alot of data from prospective students using Freedom of Information (FoI) requests to get the data out of the university. I have found one such dataset here, which we will use. This dataset contains data from 2017 to 2019 on Home (i.e. from the UK) Medicine candidiates, with the following data:

1. What year they are applying in (2017, 2018 or 2019)

2. Which college they are making their primary application to (e.g. King's, St John's, Trinity)

3. Their BMAT Section 1 score

4. Their BMAT Section 2 score

5. Whether they recieved an offer from the college they applied to i.e. ignoring pool offers. 

We can use this to ask the following question:

**"Given a particular BMAT score, what is the probability I will get an offer from Cambridge?"**

# Looking at data

As always, before starting any analysis work, let's first take a look at the data. 

We have information from `r nrow(df)` applicants to `r length(unique(df$College))` colleges in the 2017, 2018 and 2019 admission cycles. You can take a look at the dataset below.

```{r Original Dataframe}
df %>%
  select(Year:Offer)
```

Let's also try plotting out some of this to get a better sense of the dataset. First, let's plot out the distribution of BMAT scores, and see if there a difference between those who got offers and those who didn't.

```{r Distribution of Scores by Offer}
df %>%
  select(Year:Offer) %>%
  gather(BMAT_Section_1, BMAT_Section_2, key = "Section", value = "Mark") %>%
  #ggplot(aes(x = Offer, y = Mark, fill = Offer)) +
  #geom_violin() +
  ggplot(aes(x = Mark, fill = Offer)) +
  geom_density(alpha = 0.5) +
  facet_grid(rows = vars(Year), cols = vars(Section)) +
  scale_fill_brewer(type = "qual") +
  ggtitle("BMAT Scores by those who recieved Offers", subtitle = "Unsurprisingly, those who got offers got higher scores on average")
```

So unsurprisingly, there is! But we can see there is a clear overlap between the two groups: there are plenty of students who got offers (are in the purple distribution) who got lower offers than those who were rejected. 

But we also know that this is looking across all colleges: we know acceptance rates vary loads across colleges.

```{r Offer Rates by College and Year}
df %>%
  group_by(College, Year) %>%
  summarise(
    total_offers = sum(Offer),
    total_applicants = n(),
    offer_fraction = total_offers / total_applicants
  ) %>%
  mutate(`# Offers by College` = sum(total_offers), mean_offer_fraction = sum(total_offers) / sum(total_applicants)) %>%
  ungroup() %>%
  mutate(grand_offer_fraction = sum(total_offers) / sum(total_applicants)) %>%
  mutate(
    College = fct_reorder(College, mean_offer_fraction),
    Year = as.character(Year)
    ) %>%
  ggplot(aes(y = College, x = offer_fraction, colour = Year)) +
  geom_vline(aes(xintercept = grand_offer_fraction), colour = "grey", lty = 2) +
  geom_point(aes(x = mean_offer_fraction, size = `# Offers by College`), colour = "black") +
  geom_point() +
  scale_x_continuous(labels = scales::percent) +
  xlab("Percentage Of Applicants Given Offers") +
  scale_color_brewer(type = "qual") +
  ggtitle("Acceptance Rates by Cambridge college")
```

So we can see that we may need to take into account which college you are applying to, in order to improve our model - we will see if this is the case once we have our results!

# Data Processing

```{r Forming the Training and Test Datasets}

df <- df %>%
  mutate(dataset = ifelse(Year %in% c("2017", "2018"), "test", "train")) %>%
  mutate(Score = (BMAT_all - mean(BMAT_all)) / sd(BMAT_all)) %>%
  mutate(College_ID = as.numeric(as.factor(College))) %>%
  ungroup() %>%
  select(-dataset)

train <- filter(df, Year %in% c(2017, 2018))
test <- filter(df, Year == 2019)


```

Now let us start preparing for some modelling. We are going to use the sequential nature of our dataset to use the 2017/8 data to train our model, and the 2019 model to test to see how effective it is. The reason why we some data aside - rather than using all of it - is that we want to know how effective our predictions will be on data we *haven't* seen. The model hasn't seen the 2019 data, so we can use to see how good our model is. 
We are splitting them chronologically because we want to see how our model predicts things in the future: so let's do the same thing for our testing, and use the earlier years to train the model, and later years to predict. After all, if our model which is trained on data from 2017-8 can't predict the 2019 acceptance rates, it's probably going to be no use for 2020 and onwards! 

There are two other data processing steps before we start modelling. We are going to take our scores on the BMAT Section 1 and 2 and add them together to create one BMAT score. This is because these scores are highly correleated i.e. if you do well on BMAT Section 1, you will very likely do well on BMAT Section 2:

```{r Correlation between BMAT Scores}

df %>%
  ggplot(aes(BMAT_Section_1, BMAT_Section_2)) +
  geom_point() +
  xlab("BMAT Section 1") + ylab("BMAT Section 2") +
  ggtitle("Relationship between BMAT Section 1 and 2 scores", subtitle = "The two results are highly correlated")

```

This will create problems for our model[^1], so we will add the scores and just use one input to predict probabilty of getting an offer. We will also standardise the scores, by taking away the mean and dividing by the standard deviation of the distribution. Looking at our test set, you can see the effect below:

```{r}
test %>%
  rename(`Standardised BMAT Mark` = Score, `Raw BMAT Mark`= BMAT_all) %>%
  gather(contains("Mark"), key = "Measure", value = "value") %>%
  ggplot(aes(x = value, fill = Measure)) +
  geom_density() +
  stat_function(fun = dnorm, colour = "red") +
  facet_grid(rows = vars(Measure), scales = "free_x") +
  xlab("Score") +
  scale_fill_brewer(type = "qual") +
  ggtitle("Shifting BMAT Score", subtitle = "We can now reference a Standard Normal Distribution")
```

Again, the reasons are not too important [^2], but this will help us when interpreting the model we get given: we can say things like "1 SD increase in your BAMT score will increase your chances of getting in by X amount", and we can do this because we have now made sure that the underlying distribution has a standard deviation of 1 unit. It will also let us take our model and use on datasets with different means and standard deviations by first normalising them, making our model more generalisable. 

We will use the normalised score (which goes between -3 and 3 roughly) when modelling, and we will convert back to the normal BMAT score when assessing the results. Don't be alarmed when you see it: a normalised score for -3 means getting a really low score on the BMAT, 0 means getting an average score, and 3 means getting a very high score.

# Prior predictive modelling

The purpose of our analysis is to get model which for a given BMAT score, will give us a probabilty of being accepted. We will be using the logistic function. 

Let's take a look at the logistic model: brace youself:

$$ p = \frac{1}{1 + e^{-(a + b * Score)}}$$

Wew. So whilst we got our improved model benefits, we have subbed that in for a fairly nightmarish equation!

For this model, we need to estimate two parameters - $a$ and $b$

The good news that whilst the logistic model looks tricky for us, the computer can handle it without any issues, and we're going to be using the computer to do our model fitting. $a$ and $b$ have similar meanings to the linear regression equation, but they are subtly different because we no longer have a striaght line. The best way to think about the parameters is to see how the affect the shape of the line, as shown below:

```{r}
cross_df(list(x = seq(-3, 3, 0.1), a = seq(-2, 2), b = seq(5))) %>%
  mutate(y = a + b * x, p = 1 / (1 + exp(-y))) %>%
  ggplot(aes(x, p)) +
  geom_vline(xintercept = 0, lty = 2, colour = "red") +
  geom_line() +
  facet_grid(rows = vars(a), cols = vars(b)) +
  xlab("Score") +
  ggtitle("Different parameter effects", subtitle = "Rows = a, Columns = b")
```

The $a$ parameter moves the line left and right: a high $a$ means the student with an average BMAT score (shown as a normalised score of 0 and the red dashed line) will have a high probabilty of getting in - look where the red line crosses the logistic curve higher and higher as $a$ gets bigger towards the bottom. The $b$ value indicates the steepness of the line: high $b$ values on the right means a small change of marks in the middle increases your chance of getting in by alot, whilst those with already high or low scores will see very little change.

TODO: Add in section on varying intercepts, or remove influence of college in above chart. 

# Prior predictive modelling for parameters

So now we have two potential models: a straight line model and a logistic model. Now back at GCSE/A-Level, we would plot out our data and try to draw a straight line through it, and estimate the slope and intercept. Whilst this is possible directly for our straight line model directly, we could transform our normalised BMAT scores using our logistic equation and to a similar routine of plotting out the points and drawing a striaght line if we were so inclined. This kind of approach is called **regression**, and our outputs will be estimates for $a$ and $b$. These esimates will let us draw the line and allow us to predict likelihood of entry into Cambridge for anyone we can get BMAT data off. 
This is approximately what our computer behind the scences will be doing if we were using a **frequentist** approach - which is often the default - and the functions `lm()` and `glm()` from R. 

Because we are using a Bayesian approach though, there's one thing we need to do in addition to the above steps: we need to tell the computer what we roughly think our paramters $a$ and $b$ will be, and then let the computer use those suggestions to develop estimates for our parameters.

There are again mathetmatical/philosophical reasons for preferring this approach, but there (again) two main practical reasons:

1. The computer will have much less uncertainty when tell us the estimates - if you've have some stats experience, that would be approximately the same as saying the confidence intervals will be smaller. This is because we are going to tell the computer what are parameters are possible, whilst a frequentist analysis doesn't, so the computer can ignore the impossible or unlikely values for the parameter and only focus on the ones we told it are possible. 

2. We are going to get to throw away the clunky language which comes with frequentist statistics, and use much clearer language: so instead of calculating a X% confidence interval, which is defined as *"the interval in which if the experiment was repeated 100 times, the parameter would lie within X times"*, we can caculate an X% compatibility interval, defined as *"the interval we are X% sure the parameter's true value lies"*, which is far more intuitive (note that these are not the same thing!). At the very least, you can stop annoying statisticians from griping about how you describe your results, which honestly counts as a win in my book. 

So to do our predictive modelling, we need to draw out what the results from our estimates for $a$ and $b$ are: if we have reasonable estimates for $a$ and $b$, then the potential relationship between BMAT score and probabilty of getting an offer will be reasonable as well.

So let's think about what reasonable line would look like:

```{r Blank modelling plot}

cross_df(list(x = seq(-3, 3, 0.1), a = rnorm(30, 0.5, 0.07), b = exp(rnorm(30, log(1/6), 0.5)))) %>%
  mutate(y = a + b * x) %>%
  unite(col = "ab", a, b) %>%
  ggplot(aes(x, y, group = ab)) +
  theme_minimal() +
  xlab("Normalised BMAT Score") + ylab("Probabilty of Offer")

```

So for our striaght-line model, we need to determine the $a$ and $b$ parameter should roughly be. We know the following things about what we want this line to look like:

1. The line should be sloping upwards, from bottom-left to top-right - there is no way that students with higher BMAT scores are less likely to get in!

2. I know that roughly 20% of applicants are offered places at Cambridge.

3. Students with about a 4/18 on the BMAT are very unlikely to get in, whilst those with 16/18 are very likely to get in

Using these three facts, I can start saying things that my distribution of $b$ needs to be positive and $a$ needs to give us an intercept where the mean BMAT score i.e. a BMAT normalised score of 0 goes through roughly 20%. We also know that we should be staying between 0 and 1 on the y-axis for nearly all of our possible normalised BMAT scores. 


```{r Prior Predictive Modelling for Logistic Regression}

# a ~ Normal(-1, 1.5)
# b ~ Log-Normal(1, .25)
cross_df(list(x = seq(-3, 3, 0.1), a = rnorm(20, -1, 1.5), b = exp(rnorm(20, 1, .25)))) %>%
  mutate(y = a + b * x, p = 1 / (1 + exp(-y))) %>%
  unite(col = "ab", a, b) %>%
  ggplot(aes(x, p, group = ab)) +
  geom_line(alpha = .1) +
  theme_minimal() +
  xlab("Normalised BMAT Score") + ylab("Probabilty of Offer")


```

```{r Displaying distribution of logistic parameters}
tibble(a = rnorm(1e5, -1, 1.5), b = exp(rnorm(1e5, 1, 0.25))) %>%
  gather(a, b, key = "key", value = "value") %>%
  ggplot(aes(value, fill = key)) +
  geom_density() +
  scale_fill_brewer(type = "qual") +
  guides(fill = FALSE) +
  facet_wrap(~key) +
  ggtitle("Prior distribution of straight-line model parameters")
```

Now let's run some models!

```{r Generating Fixed-effects models, echo=FALSE}


bmat_models <- vector("list", 5)
names(bmat_models) <- list("linear", "fixed_intercept_only", "fixed_intercept_and_fixed_slope", "varying_intercept_and_fixed_slope", "varying_intercept_and_varying_slope", "adaptive_intercepts_and_adaptive_slopes")

names(bmat_models) <- list("linear", "fixed_intercept_only", "fixed_intercept_and_fixed_slope", "varying_intercept_and_fixed_slope", "varying_intercept_and_varying_slope")


# straight-line model
bmat_models[[1]] <- quap(
  alist(
    Offer <- a + b * Score,
    a ~ dnorm(0.4, 0.1),
    b ~ dlnorm(-1.8, 0.5)
  ),
  data = train
)

# Intercept only
bmat_models[[2]] <- quap(
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a,
    a ~ dnorm(-1, 1.5)
  ),
  data = train
)

# Normal fixed effects
bmat_models[[3]] <- quap(
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a + b * Score,
    a ~ dnorm(-1, 1.5),
    b ~ dlnorm(1, 0.25)
  ),
  data = train
)

# Give each college unique intercept
bmat_models[[4]] <- quap(
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a[College_ID] + b * Score,
    a[College_ID] ~ dnorm(-1, 1.5),
    b ~ dlnorm(1, 0.25)
  ),
  data = train
)

# Give each college unique slope and intercept
bmat_models[[5]] <- quap(
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a[College_ID] + b[College_ID] * Score,
    a[College_ID] ~ dnorm(-1, 1.5),
    b[College_ID] ~ dlnorm(1, 0.25)
  ),
  data = train
)

```

```{r Generating Mixed-effects models, echo=FALSE}

# Give each college unique slope and intercept with adaptive prior behind it
# bmat_models[[6]] <- ulam(
#   alist(
#     Offer ~ dbinom(1, p),
#     logit(p) <- a[College_ID] + b[College_ID] * Score,
#     a[College_ID] ~ dnorm(a_bar, 1.5),
#     a_bar ~ dnorm(1.5, 0.5),
#     b[College_ID] ~ dlnorm(b_bar, 0.25),
#     b_bar ~ dnorm(1, 0.2)
#   ),
#   data = train, chains = 4, log_lik=TRUE
# )

```

```{r}

crossed_test_df <- cross_df(list(College_ID = seq(28), Score = seq(-3, 3, 0.1)))


bmat_results <- bind_cols(
  crossed_test_df, 
  map_dfc(
    bmat_models, 
    ~apply(link(., data=crossed_test_df), MARGIN = 2, FUN = mean)
    )
)

test_binned <- test %>%
  mutate(bins = ntile(BMAT_all, n = 5)) %>%
  group_by(bins, College_ID) %>%
  summarise(BMAT_all_mean = mean(BMAT_all), fraction_offer = mean(Offer), n = n())

train_binned <- train %>%
  mutate(bins = ntile(BMAT_all, n = 5)) %>%
  group_by(bins, College_ID) %>%
  summarise(BMAT_all_mean = mean(BMAT_all), fraction_offer = mean(Offer), n = n())

```

```{r}
bmat_results %>%
  mutate(BMAT_all = Score * sd(test$BMAT_all) + mean(test$BMAT_all)) %>%
  gather(-College_ID, -Score, -BMAT_all, key = "model", value = "p") %>%
  ggplot() +
  geom_point(data = train_binned, aes(x = BMAT_all_mean, y = fraction_offer, size = n)) +
  geom_line(aes(BMAT_all, p, colour = model)) +
  scale_colour_brewer(type = "qual") +
  facet_wrap(~College_ID)
```


```{r}

bind_cols(
  test, 
  map_dfc(
    bmat_models, 
    ~apply(link(., data=test), MARGIN = 2, FUN = mean)
    )
) %>%
  gather(-(Year:College_ID), key = "model", value = "p") %>%
  mutate(pred_Offer = if_else(p > 0.5, TRUE, FALSE)) %>%
  mutate(prediction_truth = if_else(pred_Offer == Offer, TRUE, FALSE)) %>%
  group_by(model) %>%
  summarise(accuracy = mean(prediction_truth))
  #count(model, prediction_truth)

```

```{r Investigating outliers}


bmat_results %>%
  mutate(BMAT_all = Score * sd(test$BMAT_all) + mean(test$BMAT_all)) %>%
  gather(-College_ID, -Score, -BMAT_all, key = "model", value = "p") %>%
  filter(College_ID == 18) %>%
  ggplot() +
  geom_point(data = train_binned, aes(x = BMAT_all_mean, y = fraction_offer)) +
  geom_line(aes(BMAT_all, p, colour = model)) +
  scale_colour_brewer(type = "qual")

```

```{r Who are we not predicting properly?}

bmat_test_results <- bind_cols(
  test, 
  map_dfc(
    bmat_models, 
    ~apply(link(., data=test), MARGIN = 2, FUN = mean)
    )
)

bmat_test_results %>%
  mutate(difference = Offer - fixed_intercept_and_fixed_slope) %>%
  arrange(desc(abs(difference)))

```

