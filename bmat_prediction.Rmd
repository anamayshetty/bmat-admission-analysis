---
title: "Predicting BMAT Results"
author: "Anamay Shetty"
date: "12/12/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rethinking)
theme_set(theme_minimal())
knitr::opts_chunk$set(echo = FALSE)
```

```{bash Converting to R-readable format, include = FALSE}
sed -e 's| Home |,Home |g'  -e 's|Y| Y|g' -e 's| *Y| Y|g' data/raw_bmat_results_tl.txt | tr "," "\n" > data/bmat_results_tl.txt
```

```{r Read in data file, include = FALSE}
df <- read_delim("data/bmat_results_tl.txt", delim = " ", col_names = c("Domicile", "Course", "Year", "Candidate", letters[1:8]), skip = 1) %>%
  unite("x", a:h, sep = " ") %>%
  mutate(x = str_remove_all(x, " NA")) %>%
  mutate(b = str_extract(x, "[0-9](.[0-9]|) [0-9](.[0-9]|)")) %>%
  separate(b, into = c("BMAT_Section_1", "BMAT_Section_2"), sep = " ", convert = TRUE) %>%
  mutate(Offer = ifelse(str_detect(x, "Y "), TRUE, FALSE)) %>%
  mutate(x = (str_remove(x, " ?(Y|) [0-9](.[0-9]|) [0-9](.[0-9]|)"))) %>%
  rename("College" = x) %>%
  mutate(BMAT_all = BMAT_Section_1 + BMAT_Section_2) %>%
  #group_by(College, Year) %>%
  #mutate(offer_fraction = sum(Offer)/n()) %>%
  #ungroup %>%
  select(-Domicile, -Course) %>%
  filter(College != "Hughes Hall")
```

# Introduction

This document is intended to show a simple Bayesian workflow, based off the methods from McElreath's Statistical Rethinking. 

# Question

The Bio-Medial Aptitude Test (BMAT) is an admissions test sat by students applying to 8 universities in the UK, including the University of Cambridgde. It is most commonly sat in November, and consists of three sections:

1. Section 1, which is a verbal/spatial reasoning section

2. Section 2, which is a section on science and maths based on the GCSE curriculum.

3. Section 3, which is an essay section.

The entire exam takes two hours, and candidates recieve a mark of 1-9 each for Section 1 and 2, and a mark from 1-5 for Section 3, along with a mark A-E for their spelling and grammar in the essay.

Candidates often want to know what their chances of getting into Cambridge are based on their BMAT score, and this generates alot of data from prospective students using Freedom of Information (FoI) requests to get the data out of the university. I have found one such dataset here, which we will use. This dataset contains data from 2017 to 2019 on Home (i.e. from the UK) Medicine candidiates, with the following data:

1. What year they are applying in (2017, 2018 or 2019)

2. Which college they are making their primary application to (e.g. King's, St John's, Trinity)

3. Their BMAT Section 1 score

4. Their BMAT Section 2 score

5. Whether they recieved an offer from the college they applied to. 


As always, before starting any analysis work, let's first take a look at the data. 

We have information from `r nrow(df)` applicants to `r length(unique(df$College))` colleges in the 2017, 2018 and 2019 admission cycles. You can take a look at the dataset below.

That's alot of numbers though - let's instead turn some of these into graphs so we get a sense of what we're looking at. 


```{r Distribution of Scores by Offer}
df %>%
  select(Year:Offer) %>%
  gather(BMAT_Section_1, BMAT_Section_2, key = "Section", value = "Mark") %>%
  #ggplot(aes(x = Offer, y = Mark, fill = Offer)) +
  #geom_violin() +
  ggplot(aes(x = Mark, fill = Offer)) +
  geom_density(alpha = 0.5) +
  facet_grid(rows = vars(Year), cols = vars(Section)) +
  ggtitle("BMAT Scores by those who recieved Offers", subtitle = "Unsurprisingly, those who got offer got higher scores on average")
```

```{r Offer Rates by College and Year}
df %>%
  group_by(College, Year) %>%
  summarise(
    total_offers = sum(Offer),
    total_applicants = n(),
    offer_fraction = total_offers / total_applicants
  ) %>%
  mutate(no_offers_by_college = sum(total_offers), mean_offer_fraction = sum(total_offers) / sum(total_applicants)) %>%
  ungroup() %>%
  mutate(grand_offer_fraction = sum(total_offers) / sum(total_applicants)) %>%
  mutate(
    College = fct_reorder(College, mean_offer_fraction),
    Year = as.character(Year)
    ) %>%
  ggplot(aes(y = College, x = offer_fraction, colour = Year)) +
  geom_vline(aes(xintercept = grand_offer_fraction), colour = "grey", lty = 2) +
  geom_point(aes(x = mean_offer_fraction, size = no_offers_by_college), colour = "black") +
  geom_point() +
  scale_x_continuous(labels = scales::percent) +
  xlab("Percentage Of Applicants Given Offers") +
  scale_color_brewer(type = "qual")
  
```

To approach a statical problem, we need:

1. A question to answer

2. The appropriate data to answer that question

3. A statistical model to answer the question.

Now we have our questions:

"*what is the probabilty I am going to get into Cambridge based on my BMAT mark?*"

We have our BMAT score, which we are 

```{r}
df %>%
  select(Candidate, Offer, contains("BMAT")) %>%
  gather(contains("BMAT"), key = "Predictor", value = "Score") %>%
  group_by(Predictor) %>%
  nest
```

# Developing the model

Prior predictive modelling allows us to develop our model and understand some of our assumptions. 

Now let us start preparing for some modelling. We are going to use the sequential nature of our dataset to use the 2017/8 data to train our model, and the 2019 model to test to see how effective it is. 

The reason why we split into years to document dataset shift. 


```{r Forming the Training and Test Datasets}

df <- df %>%
  mutate(dataset = ifelse(Year %in% c("2017", "2018"), "test", "train")) %>%
  mutate(Score = (BMAT_all - mean(BMAT_all)) / sd(BMAT_all)) %>%
  mutate(College_ID = as.numeric(as.factor(College))) %>%
  ungroup() %>%
  select(-Candidate, -College, -dataset)

train <- filter(df, Year %in% c(2017, 2018))
test <- filter(df, Year == 2019)


```

The purpose of our analysis is to get a formula which will allow us to map between 

```{r Linear vs Logistic Model}
tibble(x = seq(2, 18, 0.01), linear = (x - 2)/16, logistic = 1 / (1 + exp(-(-10 + x)))) %>%
  gather(linear, logistic, key = "form", value = "y") %>%
  ggplot(aes(x, y, colour = form)) +
  geom_line() +
  xlab("Score") + ylab("Probabilty of Getting Offer")
```

```{r Prior Predictive Modelling for Linear Regression}

cross_df(list(x = seq(-3, 3, 0.1), a = rnorm(20, 0.5, 0.2), b = exp(rnorm(20, log(1/6), 0.2)))) %>%
  mutate(y = a + b * x) %>%
  unite(col = "ab", a, b) %>%
  ggplot(aes(x, y, group = ab)) +
  geom_line(alpha = .1) +
  theme_minimal()


```


```{r Prior Predictive Modelling for Logistic Regression}

# a ~ Normal(-1, 1.5)
# b ~ Log-Normal(1, .25)
cross_df(list(x = seq(-3, 3, 0.1), a = rnorm(20, -1, 1.5), b = exp(rnorm(20, 1, .25)))) %>%
  mutate(y = a + b * x, p = 1 / (1 + exp(-y))) %>%
  unite(col = "ab", a, b) %>%
  ggplot(aes(x, p, group = ab)) +
  geom_line(alpha = .1) +
  theme_minimal()


```

TODO: Add in prior predictive simulations for the variable and adaptive priors

```{r Generating Fixed-effects models}


bmat_models <- vector("list", 5)
names(bmat_models) <- list("linear", "intercept_only", "fixed_intercept_and_slopes", "varying_intercept_and_fixed_slope", "adaptive_intercepts_and_slopes")
coefficient_lists <- list(
  alist(
    Offer <- a + b * Score,
    a ~ dnorm(0.5, 0.2),
    b ~ dlnorm(log(1/6), 0.2)
  ),
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a,
    a ~ dnorm(-1, 1.5)
  ),
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a + b * Score,
    a ~ dnorm(-1, 1.5),
    b ~ dlnorm(1, 0.25)
  ),
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a[College_ID] + b * Score,
    a[College_ID] ~ dnorm(-1, 1.5),
    b ~ dlnorm(1, 0.25)
  ),
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a[College_ID] + b * Score,
    a[College_ID] ~ dnorm(a_bar, 1.5),
    a_bar ~ dnorm(1.5, 0.5),
    b ~ dlnorm(1, 0.25)
  )
)

model_v1 <- quap(
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a,
    a ~ dnorm(-1, 1.5)
  ),
  data = train
)

model_v2 <- quap(
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a + b * Score,
    a ~ dnorm(-1, 1.5),
    b ~ dlnorm(1, 0.25)
  ),
  data = train
)

model_v3 <- quap(
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a[College_ID] + b * Score,
    a[College_ID] ~ dnorm(-1, 1.5),
    b ~ dlnorm(1, 0.25)
  ),
  data = train
)

```

```{r Generating Mixed-effects models}
model_v4 <- ulam(
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a[College_ID] + b * Score,
    a[College_ID] ~ dnorm(a_bar, 1.5),
    a_bar ~ dnorm(1.5, 0.5),
    b ~ dlnorm(1, 0.25)
  ),
  data = train, chains = 4, log_lik=TRUE
)
```

```{r}
results <- bind_cols(
  test, 
  map_dfc(
    list(
      intercept_no_slope = model_v1, 
      constant_intercept_slope = model_v2, 
      varying_intercept_slope = model_v3, 
      adaptive_intercept_slope = model_v4
      ), 
    ~apply(sim(., data=test), MARGIN = 2, FUN = mean)
    )
)
```


```{r}

results %>%
  gather(contains("slope"), key = "model", value = "p") %>%
  mutate(pred_Offer = if_else(p > 0.5, TRUE, FALSE)) %>%
  mutate(prediction_truth = if_else(pred_Offer == Offer, TRUE, FALSE)) %>%
  group_by(model) %>%
  summarise(accuracy = mean(prediction_truth))
  #count(model, prediction_truth)

```


```{r}

map_dfc(
    list(
      intercept_no_slope = model_v1, 
      constant_intercept_slope = model_v2, 
      varying_intercept_slope = model_v3, 
      adaptive_intercept_slope = model_v4
      ), 
    ~apply(link(., data=cross_df(list(College_ID = seq(28), Score = seq(-3, 3, 0.1)))), MARGIN = 2, FUN = mean)
    ) %>%
  bind_cols(cross_df(list(College_ID = seq(28), Score = seq(-3, 3, 0.1)))) %>%
  mutate(BMAT_all = Score * sd(test$BMAT_all) + mean(test$BMAT_all)) %>%
  gather(contains("slope"), key = "model", value = "p") %>%
  ggplot(aes(BMAT_all, p, colour = model)) +
  geom_line() +
  facet_wrap(~College_ID)

```




```{r}
model_v3 <- ulam(
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a[College_ID] + b * Score,
    a[College_ID] ~ dnorm(a_bar, 1.5),
    a_bar ~ dnorm(1.5, 0.5),
    b ~ dlnorm(1, 0.25)
  ),
  data = train, chains = 4, log_lik=TRUE
)
```

```{r}
precis(model_v3, depth = 2)
```

`

