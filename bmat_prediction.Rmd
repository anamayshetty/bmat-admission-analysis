---
title: "Predicting BMAT Results"
author: "Anamay Shetty"
date: "12/12/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rethinking)
theme_set(theme_minimal())
knitr::opts_chunk$set(echo = FALSE)
```

```{bash Converting to R-readable format, include = FALSE}
sed -e 's| Home |,Home |g'  -e 's|Y| Y|g' -e 's| *Y| Y|g' data/raw_bmat_results_tl.txt | tr "," "\n" > data/bmat_results_tl.txt
```

```{r Read in data file, include = FALSE}
df <- read_delim("data/bmat_results_tl.txt", delim = " ", col_names = c("Domicile", "Course", "Year", "Candidate", letters[1:8]), skip = 1) %>%
  unite("x", a:h, sep = " ") %>%
  mutate(x = str_remove_all(x, " NA")) %>%
  mutate(b = str_extract(x, "[0-9](.[0-9]|) [0-9](.[0-9]|)")) %>%
  separate(b, into = c("BMAT_Section_1", "BMAT_Section_2"), sep = " ", convert = TRUE) %>%
  mutate(Offer = ifelse(str_detect(x, "Y "), TRUE, FALSE)) %>%
  mutate(x = (str_remove(x, " ?(Y|) [0-9](.[0-9]|) [0-9](.[0-9]|)"))) %>%
  rename("College" = x) %>%
  mutate(BMAT_all = BMAT_Section_1 + BMAT_Section_2) %>%
  #group_by(College, Year) %>%
  #mutate(offer_fraction = sum(Offer)/n()) %>%
  #ungroup %>%
  select(-Domicile, -Course) %>%
  filter(College != "Hughes Hall")
```

# Introduction

We have also removed Hughe's Hall, as there is only one record and it not present every eyar. We will think about Hughes' Hall in a bit. 

The Bio-Medial Aptitude Test (BMAT) is an admissions test sat by students applying to X universities. 
It contains 3 components:

1. Section 1, which is a verbal/spatial reasoning section (1-9)

2. Section 2, which is a section on science and maths (1-9)

3. Section 3, which is an essay section.

The BMAT is an important indicator of success in admissions, so we will be using a data from a FOI request to predict likelihood of admission, and using it as a based to disucuss a Baeysian workflow. 


As always, before starting any analysis work, let's first take a look at the data. 

We have information from `r nrow(df)` applicants to `r length(unique(df$College))` colleges in the 2017, 2018 and 2019 admission cycles. You can take a look at the dataset below.

That's alot of numbers though - let's instead turn some of these into graphs so we get a sense of what we're looking at. 


```{r Distribution of Scores by Offer}
df %>%
  select(Year:Offer) %>%
  gather(BMAT_Section_1, BMAT_Section_2, key = "Section", value = "Mark") %>%
  #ggplot(aes(x = Offer, y = Mark, fill = Offer)) +
  #geom_violin() +
  ggplot(aes(x = Mark, fill = Offer)) +
  geom_density(alpha = 0.5) +
  facet_grid(rows = vars(Year), cols = vars(Section)) +
  ggtitle("BMAT Scores by those who recieved Offers", subtitle = "Unsurprisingly, those who got offer got higher scores on average")
```

```{r Offer Rates by College and Year}
df %>%
  group_by(College, Year) %>%
  summarise(
    total_offers = sum(Offer),
    total_applicants = n(),
    offer_fraction = total_offers / total_applicants
  ) %>%
  mutate(no_offers_by_college = sum(total_offers), mean_offer_fraction = sum(total_offers) / sum(total_applicants)) %>%
  ungroup() %>%
  mutate(grand_offer_fraction = sum(total_offers) / sum(total_applicants)) %>%
  mutate(
    College = fct_reorder(College, mean_offer_fraction),
    Year = as.character(Year)
    ) %>%
  ggplot(aes(y = College, x = offer_fraction, colour = Year)) +
  geom_vline(aes(xintercept = grand_offer_fraction), colour = "grey", lty = 2) +
  geom_point(aes(x = mean_offer_fraction, size = no_offers_by_college), colour = "black") +
  geom_point() +
  scale_x_continuous(labels = scales::percent) +
  xlab("Percentage Of Applicants Given Offers") +
  scale_color_brewer(type = "qual")
  
```

To approach a statical problem, we need:

1. A question to answer

2. The appropriate data to answer that question

3. A statistical model to answer the question.

Now we have our questions:

"*what is the probabilty I am going to get into Cambridge based on my BMAT mark?*"

We have our 

```{r}
df %>%
  select(Candidate, Offer, contains("BMAT")) %>%
  gather(contains("BMAT"), key = "Predictor", value = "Score") %>%
  group_by(Predictor) %>%
  nest
```

# Developing the model

Prior predictive modelling allows us to develop our model and understand some of our assumptions. 

Now let us start preparing for some modelling. We are going to use the sequential nature of our dataset to use the 2017/8 data to train our model, and the 2019 model to test to see how effective it is. 

The reason why we split into years to document dataset shift. 


```{r Forming the Training and Test Datasets}

df <- df %>%
  mutate(dataset = ifelse(Year %in% c("2017", "2018"), "test", "train")) %>%
  mutate(Score = (BMAT_all - mean(BMAT_all)) / sd(BMAT_all)) %>%
  mutate(College_ID = as.numeric(as.factor(College))) %>%
  ungroup() %>%
  select(-Candidate, -College, -dataset)

train <- filter(df, Year %in% c(2017, 2018))
test <- filter(df, Year == 2019)


```

The purpose of our analysis is to get a formula which will allow us to map between 

```{r Prior Prediction Simulation}
tibble(x = seq(2, 18, 0.01), linear = (x - 2)/16, logistic = 1 / (1 + exp(-(-10 + x)))) %>%
  gather(linear, logistic, key = "form", value = "y") %>%
  ggplot(aes(x, y, colour = form)) +
  geom_line() +
  xlab("Score") + ylab("Probabilty of Getting Offer")
```


```{r Generating Fixed-effects models}
model_v1 <- quap(
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a,
    a ~ dnorm(-1, 1.5)
  ),
  data = train
)

model_v2 <- quap(
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a + b * Score,
    a ~ dnorm(-1, 1.5),
    b ~ dlnorm(1, 0.25)
  ),
  data = train
)

model_v3 <- quap(
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a[College_ID] + b * Score,
    a[College_ID] ~ dnorm(-1, 1.5),
    b ~ dlnorm(1, 0.25)
  ),
  data = train
)

```

```{r Generating Mixed-effects models}
model_v4 <- ulam(
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a[College_ID] + b * Score,
    a[College_ID] ~ dnorm(a_bar, 1.5),
    a_bar ~ dnorm(1.5, 0.5),
    b ~ dlnorm(1, 0.25)
  ),
  data = train, chains = 4, log_lik=TRUE
)
```

```{r}
results <- bind_cols(
  test, 
  map_dfc(
    list(
      intercept_no_slope = model_v1, 
      constant_intercept_slope = model_v2, 
      varying_intercept_slope = model_v3, 
      adaptive_intercept_slope = model_v4
      ), 
    ~apply(sim(., data=test), MARGIN = 2, FUN = mean)
    )
)
```


```{r}

results %>%
  gather(contains("slope"), key = "model", value = "p") %>%
  mutate(pred_Offer = if_else(p > 0.5, TRUE, FALSE)) %>%
  mutate(prediction_truth = if_else(pred_Offer == Offer, TRUE, FALSE)) %>%
  group_by(model) %>%
  summarise(accuracy = mean(prediction_truth))
  #count(model, prediction_truth)

```


```{r}

map_dfc(
    list(
      intercept_no_slope = model_v1, 
      constant_intercept_slope = model_v2, 
      varying_intercept_slope = model_v3, 
      adaptive_intercept_slope = model_v4
      ), 
    ~apply(link(., data=cross_df(list(College_ID = seq(28), Score = seq(-3, 3, 0.1)))), MARGIN = 2, FUN = mean)
    ) %>%
  bind_cols(cross_df(list(College_ID = seq(28), Score = seq(-3, 3, 0.1)))) %>%
  mutate(BMAT_all = Score * sd(test$BMAT_all) + mean(test$BMAT_all)) %>%
  gather(contains("slope"), key = "model", value = "p") %>%
  ggplot(aes(BMAT_all, p, colour = model)) +
  geom_line() +
  facet_wrap(~College_ID)

```




```{r}
model_v3 <- ulam(
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a[College_ID] + b * Score,
    a[College_ID] ~ dnorm(a_bar, 1.5),
    a_bar ~ dnorm(1.5, 0.5),
    b ~ dlnorm(1, 0.25)
  ),
  data = train, chains = 4, log_lik=TRUE
)
```

```{r}
precis(model_v3, depth = 2)
```

`

