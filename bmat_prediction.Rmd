---
title: "Predicting BMAT Results"
author: "Anamay Shetty"
date: "12/12/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rethinking)
theme_set(theme_minimal())
knitr::opts_chunk$set(echo = FALSE)
```

```{bash Converting to R-readable format, include = FALSE}
sed -e 's| Home |,Home |g'  -e 's|Y| Y|g' -e 's| *Y| Y|g' data/raw_bmat_results_tl.txt | tr "," "\n" > data/bmat_results_tl.txt
```

```{r Read in data file, include = FALSE}
df <- read_delim("data/bmat_results_tl.txt", delim = " ", col_names = c("Domicile", "Course", "Year", "Candidate", letters[1:8]), skip = 1) %>%
  unite("x", a:h, sep = " ") %>%
  mutate(x = str_remove_all(x, " NA")) %>%
  mutate(b = str_extract(x, "[0-9](.[0-9]|) [0-9](.[0-9]|)")) %>%
  separate(b, into = c("BMAT_Section_1", "BMAT_Section_2"), sep = " ", convert = TRUE) %>%
  mutate(Offer = ifelse(str_detect(x, "Y "), TRUE, FALSE)) %>%
  mutate(x = (str_remove(x, " ?(Y|) [0-9](.[0-9]|) [0-9](.[0-9]|)"))) %>%
  rename("College" = x) %>%
  mutate(BMAT_all = BMAT_Section_1 + BMAT_Section_2) %>%
  #group_by(College, Year) %>%
  #mutate(offer_fraction = sum(Offer)/n()) %>%
  #ungroup %>%
  select(-Domicile, -Course) %>%
  filter(College != "Hughes Hall")
```

We have also removed Hughe's Hall, as there is only one record and it not present every eyar. We will think about Hughes' Hall in a bit. 

The Bio-Medial Aptitude Test (BMAT) is an admissions test sat by students applying to X universities. 
It contains 3 components:

1. Section 1, which is a verbal/spatial reasoning section (1-9)

2. Section 2, which is a section on science and maths (1-9)

3. Section 3, which is an essay section.

The BMAT is an important indicator of success in admissions, so we will be using a data from a FOI request to predict likelihood of admission, and using it as a based to disucuss a Baeysian workflow. 


As always, before starting any analysis work, let's first take a look at the data. 

We have information from `r nrow(df)` applicants to `r length(unique(df$College))` colleges in the 2017, 2018 and 2019 admission cycles. You can take a look at the dataset below.

That's alot of numbers though - let's instead turn some of these into graphs so we get a sense of what we're looking at. 


```{r Distribution of Scores by Offer}
df %>%
  select(Year:Offer) %>%
  gather(BMAT_Section_1, BMAT_Section_2, key = "Section", value = "Mark") %>%
  #ggplot(aes(x = Offer, y = Mark, fill = Offer)) +
  #geom_violin() +
  ggplot(aes(x = Mark, fill = Offer)) +
  geom_density(alpha = 0.5) +
  facet_grid(rows = vars(Year), cols = vars(Section)) +
  ggtitle("BMAT Scores by those who recieved Offers", subtitle = "Unsurprisingly, those who got offer got higher scores on average")
```

```{r Offer Rates by College and Year}
df %>%
  group_by(College, Year) %>%
  summarise(
    total_offers = sum(Offer),
    total_applicants = n(),
    offer_fraction = total_offers / total_applicants
  ) %>%
  mutate(no_offers_by_college = sum(total_offers), mean_offer_fraction = sum(total_offers) / sum(total_applicants)) %>%
  ungroup() %>%
  mutate(grand_offer_fraction = sum(total_offers) / sum(total_applicants)) %>%
  mutate(
    College = fct_reorder(College, mean_offer_fraction),
    Year = as.character(Year)
    ) %>%
  ggplot(aes(y = College, x = offer_fraction, colour = Year)) +
  geom_vline(aes(xintercept = grand_offer_fraction), colour = "grey", lty = 2) +
  geom_point(aes(x = mean_offer_fraction, size = no_offers_by_college), colour = "black") +
  geom_point() +
  scale_x_continuous(labels = scales::percent) +
  xlab("Percentage Of Applicants Given Offers") +
  scale_color_brewer(type = "qual")
  
```

To approach a statical problem, we need:

1. A question to answer

2. The appropriate data to answer that question

3. A statistical model to answer the question.

Now we have our questions:

"*what is the probabilty I am going to get into Cambridge based on my BMAT mark?*"

We have our 

```{r}
df %>%
  select(Candidate, Offer, contains("BMAT")) %>%
  gather(contains("BMAT"), key = "Predictor", value = "Score") %>%
  group_by(Predictor) %>%
  nest
```

Now let us start preparing for some modelling. We are going to use the sequential nature of our dataset to use the 2017/8 data to train our model, and the 2019 model to test to see how effective it is. 
The reason why we split into years to document dataset shift. 


```{r Forming the Training and Test Datasets}

df <- df %>%
  mutate(dataset = ifelse(Year %in% c("2017", "2018"), "test", "train")) %>%
  mutate(Score = (BMAT_all - mean(BMAT_all)) / sd(BMAT_all)) %>%
  mutate(College_ID = as.numeric(as.factor(College))) %>%
  ungroup() %>%
  select(-Candidate, -College, -dataset)

train <- filter(df, Year %in% c(2017, 2018))
test <- filter(df, Year == 2019)


```

```{r Prior Prediction Simulation}

test %>%
  ggplot(Score, Offer)

```


```{r Generating Fixed-effects model}
model_v1 <- quap(
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a,
    a ~ dnorm(-1, 1.5)
  ),
  data = train
)

model_v2 <- quap(
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a + b * Score,
    a ~ dnorm(-1, 1.5),
    b ~ dlnorm(1, 0.25)
  ),
  data = train
)
```



```{r}

test %>%
  mutate(prediction = apply(sim(model_v1, data=test), MARGIN = 2, FUN = mean))


```




```{r}
model_v3 <- ulam(
  alist(
    Offer ~ dbinom(1, p),
    logit(p) <- a[College_ID] + b * Score,
    a[College_ID] ~ dnorm(a_bar, 1.5),
    a_bar ~ dnorm(1.5, 0.5),
    b ~ dlnorm(1, 0.25)
  ),
  data = train, chains = 4, log_lik=TRUE
)
```

```{r}
precis(model_v3, depth = 2)
```


```{r}
data(reedfrogs)
d <- reedfrogs
# make the tank cluster variable 
d$tank <- 1:nrow(d)
dat <- list(
  S = d$surv,
  N = d$density,
  tank = d$tank )

m13.2 <- ulam( alist(
  S ~ dbinom( N , p ) ,
  logit(p) <- a[tank] ,
  a[tank] ~ dnorm( a_bar , sigma ) , a_bar ~ dnorm( 0 , 1.5 ) ,
  sigma ~ dexp( 1 )
  ), data=dat , chains=4 , log_lik=TRUE )
```

```{r}
sort(unique(train$College_ID))
```


```{r}
library(rethinking)

data(chimpanzees)
d <- chimpanzees
d$treatment <- 1 + d$prosoc_left + 2*d$condition

dat_list <- list(
  pulled_left = d$pulled_left,
  actor = d$actor,treatment = as.integer(d$treatment)
  )
# particles in 11-dimensional space

m11.4 <- ulam(
  alist(
    pulled_left ~ dbinom( 1 , p ) ,
    logit(p) <- a[actor] + b[treatment] ,
    a[actor] ~ dnorm( 0 , 1.5 ),
    b[treatment] ~ dnorm( 0 , 0.5 )
    ) ,
  data=dat_list , chains=4 )

precis( m11.4 , depth=2 )

```

