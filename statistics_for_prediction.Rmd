---
title: "Statistics for Prediction"
author: "Anamay Shetty"
date: "12/12/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rethinking)
theme_set(theme_minimal())
knitr::opts_chunk$set(echo = FALSE)
```

```{bash Converting to R-readable format, include = FALSE}
sed -e 's| Home |,Home |g'  -e 's|Y| Y|g' -e 's| *Y| Y|g' data/raw_bmat_results_tl.txt | tr "," "\n" > data/bmat_results_tl.txt
```

```{r Read in data file, include = FALSE}
df <- read_delim("data/bmat_results_tl.txt", delim = " ", col_names = c("Domicile", "Course", "Year", "Candidate", letters[1:8]), skip = 1) %>%
  unite("x", a:h, sep = " ") %>%
  mutate(x = str_remove_all(x, " NA")) %>%
  mutate(b = str_extract(x, "[0-9](.[0-9]|) [0-9](.[0-9]|)")) %>%
  separate(b, into = c("BMAT_Section_1", "BMAT_Section_2"), sep = " ", convert = TRUE) %>%
  mutate(Offer = ifelse(str_detect(x, "Y "), TRUE, FALSE)) %>%
  mutate(x = (str_remove(x, " ?(Y|) [0-9](.[0-9]|) [0-9](.[0-9]|)"))) %>%
  rename("College" = x) %>%
  mutate(BMAT_all = BMAT_Section_1 + BMAT_Section_2) %>%
  # This is to deal with pesky floating point problems
  mutate(BMAT_all = as.integer(BMAT_all * 10) / 10) %>%
  select(Candidate, Year, Offer, BMAT_all)

test_df <- filter(df, Year == 2019)
train_df <- filter(df, Year != 2019)

```

Statistics is a common feature of many science courses. I see statistics as solving two questions in science:

1. Using current data and knowledge, can we predict an outcome in other situations?

2. Using current data and knowledge, can we understand the underlying mechanisms which cause our outcomes of interest?

In a more concrete sense, question 1 in prediction - "What will the effects of giving drug A to person X be?" - whilst question 2 is a question of inference - "Is our scientific model of drug action correct given data about drug effects?".

These are often conflated, but I think prediction is something quite apart from inference, and this little piece is designed to really pull these apart - and ultimately show **why** they are so often conflated. We will tackle the question of inference in a separate piece. 

A bit of background. When I was first starting off as a very bor(ed/ing) Year 12/13 student just getting interested in statistics, I went and worked one of the first things I did was to download a dataset of BMAT scores and medicine Offers and do an excercise in prediction. I imagined what I would say if someone came up to me and asked:

"Anamay I have <blah> BMAT score: what's my chances of getting into Cambridge?"

### Option 1: You're not going to get in

Let's say I gave you my dataset, but without the BMAT scores. You may think that there's no way you can answer the question posed, but we can!

The average fraction of applications which lead to offers is `r round(mean(train_df$Offer), 3)`. This means that if you knew nothing about the person, you should tell them they won't get in! After all, you'll be right roughly 75% of the time.

Let's just reflect on that - even without *any* modelling or statistics, I can accurately call an applicant's result 3 out of 4 times. This is why when papers are published talking about cool their snazzy new prediction algorithm is, you shouls always make sure that they have a comparison against something basic like our blanket 'No' rule. You will be surprised how effective simple rules can be, and why simple accuracy scores often don't show the whole picture. 

To keep track as we're going along, we're going to use the 2019 year results as a yardstick to measure how well we're doing. We're going to take each rule and apply to the 2019 batch and see how many we predict correctly. 

Our first one is easy. Since `r round(mean(!test_df$Offer), 3)` of applicants in 2019 didn't get an offer, you would have a `r round(mean(!test_df$Offer), 3)` accuracy if you told every applicant you met "You're not going to get in".

### Option 2: Let me find your score in my table

But I didn't download this whole dataset for nothing. The next step up is to take an applicant's score and look up in our table. Depending on what previous candidates got in previous years, we can tell our fresh-faced sixth former what the outcome of their application will be.

Let's say our enterprising candidate got 4.9 on their BMAT and wants to know what their chance of getting in is. We can look up in our dataset what previous candidates got:

```{r}
train_df %>%
  filter(BMAT_all == 4.9)

```

We can see from the above table that one candidate got 4.9 in 2018 and they did not get in. What happens if we predict our candidate won't get in this year with 4.9 as well?

```{r}
test_df %>%
  filter(BMAT_all == 4.9)

```


You can see that you would be correct! 

Obviously this is just one example and presents two obvious problems: what do I do when there are many candidates with the same score, and what do we do when we have a candidate with a score not previously seen before?

### Option 3: Let's just match people up

Let's take that first problem on first. Let's say we meet with a candidate with a score of 12.3?

```{r}
train_df %>%
  filter(BMAT_all == 12.3) %>%
  count(Offer)

```

We can see that there are 27 candidates in 2017 and 2018 with a score of 12.3: 21 of them got an offer (yay) and 6 did not (cry).

What do we tell our candidate this year with a score of 12.3? One approach is to say because more candidates got offer than did not, that you should tell the candidate that they will get in. 

```{r}
test_df %>%
  filter(BMAT_all == 12.3)
```

We can see that there were 5 candidates in 2019 who got 12.3: 3 of them got offers and 2 did not. What would happen if we used this strategy of deciding whether candidates got in or not depending on whether more than 50% of candidates in previous years with their score got in. 

```{r}
matching_results <- test_df %>%
  left_join(train_df, by = "BMAT_all", suffix = c(".test", ".train")) %>%
  #filter(is.na(Offer.train))
  group_by(Candidate.test, Offer.test) %>%
  summarise(pred_Offer = mean(Offer.train) > 0.5) %>%
  mutate(Correct_prediction = Offer.test == pred_Offer) %>%
  ungroup %>%
  count(Correct_prediction)

matching_results
```

As we can see from the table above, we `r matching_results[2, 2]` candidates who we gave a correct response to, and `r matching_results[1, 2]` who we didn't. The `r matching_results[3, 2]` NAs refer to candidates who got a score that wasn't found in the previous two years, and so we couldn't say anything to them because we had no data!

If we ignore the NAs, our accuracy is `r round(matching_results[2, 2] / (matching_results[2, 2]+matching_results[1, 2]), 3)` which is about 6-7% better than a blanket "No" approach! This is because we are now having a look at the candidate's BMAT score, and we are learning that candidates with a high BMAT score are going to do well, and those with a low score are not.

However, we still have nothing to say to those candidates with NAs. Let's take our prediction efforts one step further. 

### Option 4: Let's start doing some linear interpolation and probabilties?

Let's start by drawing some nice graphs.

```{r}
train_df %>%
  ggplot(aes(x = BMAT_all, y = Offer)) +
  geom_point() +
  xlab("BMAT Score")
```

We can see here that we have plotted two sets of BMAT scores from 2017 and 2018: the lower dots are for unsuccessful candidates and the upper line is for successful candidates.

Now to replicate what we have done so far, we need to plot our results for *each* score, with the y-axis showing the average number of candidates with that score getting an offer. 

```{r}
train_df %>%
  group_by(BMAT_all) %>%
  summarise(Offer = mean(Offer)) %>%
  {ggplot(data = ., aes(x = BMAT_all, y = Offer)) +
  geom_point(data = filter(., BMAT_all == 12.3), colour = "red", size = 3) +
  #geom_point(data = filter(., BMAT_all == 14.1), colour = "green", size = 3) +
  geom_point(data = filter(., BMAT_all == 4.9), colour = "blue", size = 3) +
  geom_point() +
  geom_vline(xintercept = 14.7) +
  scale_y_continuous(labels = scales::percent) +
  xlab("BMAT Score") + ylab("Average Success Rate")}
  
```

Let's just pause and understand what we've drawn. I've highlighted two of the points on the graph. 

The green point corresponds to the score of 4.9, which we discussed before is our one candidate in 2018 who did not get in. Because only one person got 4.9 and they did not get in, we say that anyone who gets a score of 4.9 will never get in. 

The red points corresponds to the score of 12.3. Here we had 27 candidates, of whom 21 got in. Because 21/27 got in, we say the average success rate is 78%. Our rule from before was to say that anyone with a score where less than 50% of previous candidates got an offer was *not* going to get an offer, and vice versa.

Before we move on, let's reflect on that. Instead of telling our applicant whether they will get in or not, we can give them a *probabilty* of getting an offer: 20%, 40%, 60% etc. This naturally comes out of the averages we've been calculating and makes intuitive sense: if 3 out of 5 (60%) people in the past got an offer with a score of 12.3, then future applicants will have a 60% chance of getting an offer.

But what do we do about someone's score we haven't seen before, like someone with a score of 14.7?

```{r}
train_df %>%
  filter(BMAT_all == 14.7)
```

We can look either side of this missing score and see what are the closest results we have:

```{r}
train_df %>%
  filter(BMAT_all %in% c(14.6, 14.9))
```

We can see that either side of this score, the candidate did get in, so we can predict that someone with a score of 4.7 *also* will get in. 
This is the equivalent of drawing a line through our points, and if we have any gaps, using the line:

```{r}
train_df %>%
  group_by(BMAT_all) %>%
  summarise(Offer = mean(Offer)) %>%
  filter(BMAT_all  >= 14.5 & BMAT_all <= 15) %>%
  ggplot(aes(x = BMAT_all, y = Offer)) +
  geom_line() +
  geom_point(size = 5) +
  geom_point(data = tibble(BMAT_all = 14.7, Offer = 1), size = 5, colour = "red") +
  scale_y_continuous(labels = scales::percent, breaks = c(0.95, 1))
  
```

We can extend this out to the rest of our dataset, and fill in all avaialable gaps. Let's see what it does to our accuracy:

```{r}

do_linear_interpolation_y <- function(x1, x2, y1, y2, x) {
  
  m <- (y2 - y1) / (x2 - x1)
  c <- y1
  y <- m * (x - x1) + c
  
  return(y)
}

no_gaps_train_df <- train_df %>%
  group_by(BMAT_all) %>%
  summarise(mean = mean(Offer)) %>%
  mutate(x1 = BMAT_all, x2 = BMAT_all, y1 = mean, y2 = mean) %>%
  right_join(tibble(BMAT_all = seq(10, 180)/10), by = "BMAT_all") %>%
  fill(x1, y1, .direction = "down") %>%
  fill(x2, y2, .direction = "up") %>%
  mutate(predmean = do_linear_interpolation_y(x1, x2, y1, y2, BMAT_all)) %>%
  mutate(Offer = ifelse(is.na(mean), predmean, mean)) %>%
  select(BMAT_all, Offer) %>%
  fill(Offer, .direction = "up")

test_df %>%
  left_join(no_gaps_train_df, by = "BMAT_all", suffix = c(".test", ".train")) %>%
  mutate(Correct_prediction = Offer.test == (Offer.train > 0.5)) %>%
  count(Correct_prediction)

  

```

And we have now correctly classified out previous NAs! We can show our current progress with a graph:

```{r}

eleven_graph_intersection <- train_df %>%
  group_by(BMAT_all) %>%
  summarise(Offer = mean(Offer)) %>%
  filter(BMAT_all == 11.0) %>%
  .[[1, 2]]

train_df %>%
  group_by(BMAT_all) %>%
  summarise(Offer = mean(Offer)) %>%
  ggplot(aes(x = BMAT_all, y = Offer)) +
  geom_line() +
  geom_segment(x = 11, y = 0, xend = 11, yend = 0.3636364, colour = "red", lty = 2) +
  geom_segment(x = 0, y = 0.3636364, xend = 11, yend = 0.3636364, colour = "red", lty = 2) +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(breaks = seq(1, 18))  +
  xlab("BMAT Score") + ylab("Average Success Rate")

```

For each potential BMAT score, we can go and see where it intersects the line, and then read off the average success score: for example, we can ~36% of candidates in 2017/8 with a BMAT score of 11 got an offer, so we will tell a student with 11 that they have a 36% chance of getting in i.e. they will be rejected.

### Option 5: Time for a model?

Now we have a comprehensive model, and we can just leave it there - after all, we now can tell any candidate what chance they have of getting in, and we didn't have to do any stats!

I wanted to walk through all of this to show you how far we could get just using some basic problem solving. You can consider what we have to be an *algorithm*, a way of converting BMAT scores into a prediction of success or not. The next step will show us some statistics (finally), but I wanted to show it as a natural progression of our problem solving.

Let's take another look at out graph, and I've highlighted a few points:

```{r}

train_df %>%
  group_by(BMAT_all) %>%
  summarise(Offer = mean(Offer)) %>%
  {ggplot(data = ., aes(x = BMAT_all, y = Offer)) +
  geom_hline(yintercept = 0.5, colour = "grey", lty = 2) +
  geom_point(data = filter(., BMAT_all == 11.6), colour = "red", size = 3) +
  geom_point(data = filter(., BMAT_all == 11.9), colour = "purple", size = 3) +
  geom_point(data = filter(., BMAT_all == 16.3), colour = "green", size = 3) +
  geom_point(data = filter(., BMAT_all == 17), colour = "blue", size = 3) +
  geom_line() +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(breaks = seq(1, 18))  +
  xlab("BMAT Score") + ylab("Average Success Rate")}

```

Let's compare candidate who got a 11.6 (red) and someone who got 11.9 (purple). Despite the fact that the person in purple got a higher score, we would have told them that they are not going to get an offer, as they fall below 50% - the grey dashed line on the graph, whilst the candidate with the lower score will be told they will get an offer.

This is made even clearer with someone who got 17 (blue). This is an *insane* score to get, and yet we not only tell them they won't get an offer, we can *guarantee* it! They after all have a predicted 0% chance! This is despite doing better than someone who got 16.3 (green), who can *guarantee* with a 100% probabilty can get in. 

We therefore have two problems - we have all of these jumps in our data, which make adjacent scores change wildly, and we are often completely of people's chances at either end. Both of these are wrong to our common-sense: people who get similar scores should have a similar chance of getting in, and people with very low/high scores should have a low/high - but not certain chance - of getting in.

So why are we getting these results? That's because our dataset has told us that candidates who get 17 will always fail:

```{r}
train_df %>%
  filter(BMAT_all == 17)
```

But we don't just have to rely on past results - we have knowledge about how the world and exams work, which our basic algorithm hasn't been told about. 

This is where modelling comes in. We want to impose some structure on the algorithm, whilst making sure that it can change with new data. The way we do this is by having an equation - which we can represent as a line - which can be used to convert between BMAT score and success rate. 

So what kind of line can we use? Our first thought would be to use a straight line. We know that any striaght line can be described by its gradient and its y-intercept, so we need to draw a striaght-line and figure out which gradient/y-intercept combination work the best:

```{r}
train_df %>%
  group_by(BMAT_all) %>%
  summarise(Offer = mean(Offer)) %>%
  ggplot(data = ., aes(x = BMAT_all, y = Offer)) +
  geom_point() +
  stat_function(fun = function(x) x / 17 - 1/17, colour = "red") +
  stat_function(fun = function(x) x / 16 - 1/6, colour = "red") +
  stat_function(fun = function(x) x / 18 + 1/8, colour = "red") +
  scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1)) +
  scale_x_continuous(breaks = seq(1, 18))  +
  xlab("BMAT Score") + ylab("Average Success Rate")

```

I have drawn a few examples of straight lines we can use for our data. We can see that they do not fit very well and some do not make sense: the top line predicts a probabilty of greater than 100% with a score greter than 16!

This is a problem with using a striaght line - because these lines stretch onwards forever, they will run into these problems. What we need is a kind of line which caps off at 0 and 100%. Luckily, we have such a line, one described by the **logistic distribution**:

```{r}

no_gaps_train_df %>%
  ggplot(data = ., aes(x = BMAT_all, y = Offer)) +
  geom_point() +
  stat_function(fun = plogis, args = list(location = 10, scale = 1), colour = "red") +
  stat_function(fun = plogis, args = list(location = 11, scale = 2), colour = "red") +
  stat_function(fun = plogis, args = list(location = 12, scale = 0.5), colour = "red") +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(breaks = seq(1, 18))  +
  xlab("BMAT Score") + ylab("Average Success Rate")

```

Now we have a much better line: we will never get an answer bigger than 100% or less than 0 for our average success rate. It also means similar scores will have similar chances of getting in, and those with low/high scores will have a low/high chance of getting in. In general, for problems where we are trying to predict the chance of something happening between 0 and 100%, we would want to use a **logistic distribution** line[^1].

[^1]: This is a simplistic reason for choosing between different linear models for differnet applications. A more satisfying discussion can be found in McElreath's Statistical Rethinking when he discusses maximum entropy distributions and general linear models.

Like a striaght-line graph, where we wanted to estimate the slope and gradient, we also want to calculate the slope and gradient of the **logistic distribution** lines.

The slope is how steep our line is going to be - the bigger the slope, the more your chance of getting in changes.

The intercept is the BMAT score needed to have a 50% chance of getting into Cambridge. 

We can get a computer to chose which slope and intercept will create a line which best matches our data. Let's see how it's done:

```{r}

bmat_model <- glm(Offer ~ BMAT_all, family = "binomial", data = train_df)

intercept <- -bmat_model$coefficients[[1]]

slope <- bmat_model$coefficients[[2]]

```

```{r}

no_gaps_train_df %>%
  ggplot(data = ., aes(x = BMAT_all, y = Offer)) +
  geom_line() +
  stat_function(fun = plogis, args = list(location = intercept, scale = slope), colour = "red") +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(breaks = seq(1, 18))  +
  xlab("BMAT Score") + ylab("Average Success Rate")

```

Nice! We can see that our line does a pretty good job of mimicking our previous attempt to match BMAT score to success rate, whilst being far smoother. However, does this do any better than our original method of looking up the results in the table and then telling the applicant their score? Let's remind ourselves of the success of the old model:

```{r}

head(predict(bmat_model, type = "response", newdata = test_df))

model_results <- no_gaps_train_df %>%
  mutate(Offer = Offer > 0.5) %>%
  right_join(test_df, by = "BMAT_all", suffix = c(".empirical_model", ".truth")) %>%
  mutate(Offer.logistic_model = predict(bmat_model, type = "response", newdata = test_df) > 0.5)

model_results %>%
  mutate(Correct_prediction = Offer.truth == Offer.empirical_model) %>%
  count(Correct_prediction)

```

And let's see how our new model does:

```{r}
model_results %>%
  mutate(Correct_prediction = Offer.truth == Offer.logistic_model) %>%
  count(Correct_prediction)
```

And it turns out that our logistic model did much better: 84% accuracy vs 81% accuracy, with 30 more people given a correct prediction.

### Reflection

Let's think about what just happened there. We went from using a simple model just looking at past years to one using a statistical model, where we come up with a particular line, and we estimate a slope and interecpt. We can see that the stistical model is better, because it imposes some sensible rules e.g. people with higher scores should have a better chance of getting in.

This is to me *the main reason* why in data science we use statistics. Data science could just be an excercise in collecting big datasets and calculating averages, like we did before - and it works fairly well. However, we need statistical models to improve our accuracy because we need a way to tell our algorithms certain truths about the world.

This idea is often thought about as either regularisation (more common in frequentist statistics) or priors (more common in Bayesian), but I think this point of using *models* as a way improve accuracy purposely *not* directly using our data is under-rated, and forms the first step in understaing why we use statistics in prediction. 
